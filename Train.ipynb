{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e024c65f-6eef-4810-b7c9-92a962dc2ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: 413, Validation sequences: 104\n",
      "Created 371287 samples.\n",
      "Created 93496 samples.\n",
      "Epoch 01/100 | Train Loss: 0.318195 | Val Loss: 0.289123 | Val R²: 0.357324\n",
      "New best model found.\n",
      "Epoch 02/100 | Train Loss: 0.313204 | Val Loss: 0.287251 | Val R²: 0.358860\n",
      "New best model found.\n",
      "Epoch 03/100 | Train Loss: 0.312161 | Val Loss: 0.286377 | Val R²: 0.360720\n",
      "New best model found.\n",
      "Epoch 04/100 | Train Loss: 0.311537 | Val Loss: 0.286070 | Val R²: 0.362630\n",
      "New best model found.\n",
      "Epoch 05/100 | Train Loss: 0.311093 | Val Loss: 0.284372 | Val R²: 0.365578\n",
      "New best model found.\n",
      "Epoch 06/100 | Train Loss: 0.310717 | Val Loss: 0.284709 | Val R²: 0.364799\n",
      "Epoch 07/100 | Train Loss: 0.310496 | Val Loss: 0.284230 | Val R²: 0.365518\n",
      "New best model found.\n",
      "Epoch 08/100 | Train Loss: 0.310202 | Val Loss: 0.284443 | Val R²: 0.365322\n",
      "Epoch 09/100 | Train Loss: 0.309968 | Val Loss: 0.283747 | Val R²: 0.366857\n",
      "New best model found.\n",
      "Epoch 10/100 | Train Loss: 0.309868 | Val Loss: 0.283843 | Val R²: 0.365876\n",
      "Epoch 11/100 | Train Loss: 0.309727 | Val Loss: 0.284014 | Val R²: 0.367253\n",
      "Epoch 12/100 | Train Loss: 0.309466 | Val Loss: 0.284882 | Val R²: 0.365800\n",
      "Epoch 13/100 | Train Loss: 0.309409 | Val Loss: 0.284242 | Val R²: 0.366011\n",
      "Epoch 14/100 | Train Loss: 0.309376 | Val Loss: 0.284176 | Val R²: 0.365029\n",
      "Epoch 15/100 | Train Loss: 0.309218 | Val Loss: 0.284817 | Val R²: 0.362779\n",
      "Epoch 16/100 | Train Loss: 0.307544 | Val Loss: 0.282810 | Val R²: 0.367136\n",
      "New best model found.\n",
      "Epoch 17/100 | Train Loss: 0.307240 | Val Loss: 0.283092 | Val R²: 0.368037\n",
      "Epoch 18/100 | Train Loss: 0.307108 | Val Loss: 0.282435 | Val R²: 0.367152\n",
      "New best model found.\n",
      "Epoch 19/100 | Train Loss: 0.306884 | Val Loss: 0.282512 | Val R²: 0.366420\n",
      "Epoch 20/100 | Train Loss: 0.306762 | Val Loss: 0.282592 | Val R²: 0.368668\n",
      "Epoch 21/100 | Train Loss: 0.306626 | Val Loss: 0.282676 | Val R²: 0.368361\n",
      "Epoch 22/100 | Train Loss: 0.306467 | Val Loss: 0.282792 | Val R²: 0.366198\n",
      "Epoch 23/100 | Train Loss: 0.306290 | Val Loss: 0.282498 | Val R²: 0.369680\n",
      "Epoch 24/100 | Train Loss: 0.306116 | Val Loss: 0.282500 | Val R²: 0.367452\n",
      "Epoch 25/100 | Train Loss: 0.304806 | Val Loss: 0.282588 | Val R²: 0.368037\n",
      "Epoch 26/100 | Train Loss: 0.304487 | Val Loss: 0.282733 | Val R²: 0.367649\n",
      "Epoch 27/100 | Train Loss: 0.304199 | Val Loss: 0.282655 | Val R²: 0.367024\n",
      "Epoch 28/100 | Train Loss: 0.303921 | Val Loss: 0.282640 | Val R²: 0.367862\n",
      "Epoch 29/100 | Train Loss: 0.303722 | Val Loss: 0.282917 | Val R²: 0.367387\n",
      "Epoch 30/100 | Train Loss: 0.303462 | Val Loss: 0.282915 | Val R²: 0.367666\n",
      "Epoch 31/100 | Train Loss: 0.302494 | Val Loss: 0.283146 | Val R²: 0.366006\n",
      "Epoch 32/100 | Train Loss: 0.302183 | Val Loss: 0.283321 | Val R²: 0.364739\n",
      "Epoch 33/100 | Train Loss: 0.302009 | Val Loss: 0.283378 | Val R²: 0.364424\n",
      "Epoch 34/100 | Train Loss: 0.301731 | Val Loss: 0.283722 | Val R²: 0.364546\n",
      "Epoch 35/100 | Train Loss: 0.301534 | Val Loss: 0.283633 | Val R²: 0.364563\n",
      "Epoch 36/100 | Train Loss: 0.301242 | Val Loss: 0.283313 | Val R²: 0.364783\n",
      "Epoch 37/100 | Train Loss: 0.300487 | Val Loss: 0.283825 | Val R²: 0.362188\n",
      "Epoch 38/100 | Train Loss: 0.300271 | Val Loss: 0.283827 | Val R²: 0.362807\n",
      "Epoch 39/100 | Train Loss: 0.300058 | Val Loss: 0.284115 | Val R²: 0.362056\n",
      "Epoch 40/100 | Train Loss: 0.299870 | Val Loss: 0.284199 | Val R²: 0.361474\n",
      "Epoch 41/100 | Train Loss: 0.299680 | Val Loss: 0.284525 | Val R²: 0.360571\n",
      "Epoch 42/100 | Train Loss: 0.299666 | Val Loss: 0.284634 | Val R²: 0.360494\n",
      "Epoch 43/100 | Train Loss: 0.299122 | Val Loss: 0.284440 | Val R²: 0.360347\n",
      "Epoch 44/100 | Train Loss: 0.299028 | Val Loss: 0.284920 | Val R²: 0.359535\n",
      "Epoch 45/100 | Train Loss: 0.298916 | Val Loss: 0.284760 | Val R²: 0.359843\n",
      "Epoch 46/100 | Train Loss: 0.298710 | Val Loss: 0.284933 | Val R²: 0.358936\n",
      "Epoch 47/100 | Train Loss: 0.298654 | Val Loss: 0.284920 | Val R²: 0.359160\n",
      "Epoch 48/100 | Train Loss: 0.298593 | Val Loss: 0.284865 | Val R²: 0.358831\n",
      "Epoch 49/100 | Train Loss: 0.298311 | Val Loss: 0.284840 | Val R²: 0.359406\n",
      "Epoch 50/100 | Train Loss: 0.298297 | Val Loss: 0.284908 | Val R²: 0.358537\n",
      "Epoch 51/100 | Train Loss: 0.298201 | Val Loss: 0.285038 | Val R²: 0.358068\n",
      "Epoch 52/100 | Train Loss: 0.298089 | Val Loss: 0.285230 | Val R²: 0.357976\n",
      "Epoch 53/100 | Train Loss: 0.298015 | Val Loss: 0.285062 | Val R²: 0.358267\n",
      "Epoch 54/100 | Train Loss: 0.298026 | Val Loss: 0.285077 | Val R²: 0.358163\n",
      "Epoch 55/100 | Train Loss: 0.297870 | Val Loss: 0.285167 | Val R²: 0.357657\n",
      "Epoch 56/100 | Train Loss: 0.297783 | Val Loss: 0.285157 | Val R²: 0.357994\n",
      "Epoch 57/100 | Train Loss: 0.297796 | Val Loss: 0.285277 | Val R²: 0.357769\n",
      "Epoch 58/100 | Train Loss: 0.297882 | Val Loss: 0.285337 | Val R²: 0.357469\n",
      "Epoch 59/100 | Train Loss: 0.297792 | Val Loss: 0.285251 | Val R²: 0.357627\n",
      "Epoch 60/100 | Train Loss: 0.297798 | Val Loss: 0.285316 | Val R²: 0.357418\n",
      "Epoch 61/100 | Train Loss: 0.297648 | Val Loss: 0.285350 | Val R²: 0.357289\n",
      "Epoch 62/100 | Train Loss: 0.297601 | Val Loss: 0.285221 | Val R²: 0.357399\n",
      "Epoch 63/100 | Train Loss: 0.297572 | Val Loss: 0.285272 | Val R²: 0.357459\n",
      "Epoch 64/100 | Train Loss: 0.297584 | Val Loss: 0.285334 | Val R²: 0.357356\n",
      "Epoch 65/100 | Train Loss: 0.297485 | Val Loss: 0.285322 | Val R²: 0.357312\n",
      "Epoch 66/100 | Train Loss: 0.297588 | Val Loss: 0.285334 | Val R²: 0.357247\n",
      "Epoch 67/100 | Train Loss: 0.297555 | Val Loss: 0.285380 | Val R²: 0.357152\n",
      "Epoch 68/100 | Train Loss: 0.297434 | Val Loss: 0.285361 | Val R²: 0.357098\n",
      "Epoch 69/100 | Train Loss: 0.297423 | Val Loss: 0.285350 | Val R²: 0.357091\n",
      "Epoch 70/100 | Train Loss: 0.297409 | Val Loss: 0.285394 | Val R²: 0.356979\n",
      "Epoch 71/100 | Train Loss: 0.297468 | Val Loss: 0.285393 | Val R²: 0.357025\n",
      "Epoch 72/100 | Train Loss: 0.297462 | Val Loss: 0.285359 | Val R²: 0.357117\n",
      "Epoch 73/100 | Train Loss: 0.297456 | Val Loss: 0.285382 | Val R²: 0.357037\n",
      "Epoch 74/100 | Train Loss: 0.297325 | Val Loss: 0.285380 | Val R²: 0.356991\n",
      "Epoch 75/100 | Train Loss: 0.297396 | Val Loss: 0.285399 | Val R²: 0.356940\n",
      "Epoch 76/100 | Train Loss: 0.297431 | Val Loss: 0.285426 | Val R²: 0.356909\n",
      "Epoch 77/100 | Train Loss: 0.297448 | Val Loss: 0.285419 | Val R²: 0.356879\n",
      "Epoch 78/100 | Train Loss: 0.297371 | Val Loss: 0.285413 | Val R²: 0.356878\n",
      "Epoch 79/100 | Train Loss: 0.297343 | Val Loss: 0.285409 | Val R²: 0.356908\n",
      "Epoch 80/100 | Train Loss: 0.297303 | Val Loss: 0.285418 | Val R²: 0.356890\n",
      "Epoch 81/100 | Train Loss: 0.297400 | Val Loss: 0.285418 | Val R²: 0.356884\n",
      "Epoch 82/100 | Train Loss: 0.297356 | Val Loss: 0.285412 | Val R²: 0.356906\n",
      "Epoch 83/100 | Train Loss: 0.297447 | Val Loss: 0.285414 | Val R²: 0.356897\n",
      "Epoch 84/100 | Train Loss: 0.297345 | Val Loss: 0.285421 | Val R²: 0.356878\n",
      "Epoch 85/100 | Train Loss: 0.297388 | Val Loss: 0.285420 | Val R²: 0.356881\n",
      "Epoch 86/100 | Train Loss: 0.297371 | Val Loss: 0.285419 | Val R²: 0.356883\n",
      "Epoch 87/100 | Train Loss: 0.297262 | Val Loss: 0.285424 | Val R²: 0.356869\n",
      "Epoch 88/100 | Train Loss: 0.297439 | Val Loss: 0.285426 | Val R²: 0.356874\n",
      "Epoch 89/100 | Train Loss: 0.297368 | Val Loss: 0.285421 | Val R²: 0.356880\n",
      "Epoch 90/100 | Train Loss: 0.297382 | Val Loss: 0.285422 | Val R²: 0.356870\n",
      "Epoch 91/100 | Train Loss: 0.297457 | Val Loss: 0.285422 | Val R²: 0.356871\n",
      "Epoch 92/100 | Train Loss: 0.297347 | Val Loss: 0.285421 | Val R²: 0.356870\n",
      "Epoch 93/100 | Train Loss: 0.297296 | Val Loss: 0.285424 | Val R²: 0.356866\n",
      "Epoch 94/100 | Train Loss: 0.297394 | Val Loss: 0.285425 | Val R²: 0.356862\n",
      "Epoch 95/100 | Train Loss: 0.297371 | Val Loss: 0.285427 | Val R²: 0.356860\n",
      "Epoch 96/100 | Train Loss: 0.297304 | Val Loss: 0.285428 | Val R²: 0.356857\n",
      "Epoch 97/100 | Train Loss: 0.297264 | Val Loss: 0.285428 | Val R²: 0.356854\n",
      "Epoch 98/100 | Train Loss: 0.297314 | Val Loss: 0.285428 | Val R²: 0.356855\n",
      "Epoch 99/100 | Train Loss: 0.297366 | Val Loss: 0.285428 | Val R²: 0.356854\n",
      "Epoch 100/100 | Train Loss: 0.297268 | Val Loss: 0.285428 | Val R²: 0.356854\n",
      "Model saved to model.pth\n",
      "Best Val Loss: 0.282435, Best Val R²: 0.367152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333e5e06ebe74adb8260a32948e5cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Mean R² (Scorer): 0.270832\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import Parallel, delayed\n",
    "from utils import ScorerStepByStep, DataPoint\n",
    "\n",
    "#  Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#  Dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data, window_size=10, warmup=100, n_jobs=-1):\n",
    "        self.window_size = window_size\n",
    "        self.warmup = warmup\n",
    "        self.n_jobs = n_jobs\n",
    "        self.samples = self._create_samples(data)\n",
    "\n",
    "    def _process_sequence(self, seq_data):\n",
    "        seq_id, seq_df = seq_data\n",
    "        seq_values = seq_df.iloc[:, 3:].values\n",
    "        samples = []\n",
    "        for t in range(self.warmup, len(seq_values) - 1):\n",
    "            start = t - self.window_size\n",
    "            if start < 0:\n",
    "                continue\n",
    "            X = seq_values[start:t]\n",
    "            y = seq_values[t + 1]\n",
    "            samples.append((X, y))\n",
    "        return samples\n",
    "\n",
    "    def _create_samples(self, data):\n",
    "        groups = list(data.groupby(\"seq_ix\"))\n",
    "        results = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._process_sequence)(g) for g in groups\n",
    "        )\n",
    "        samples = [s for seq in results for s in seq]\n",
    "        print(f\"Created {len(samples)} samples.\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.samples[idx]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "#  GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, output_dim=None, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim or input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        last = out[:, -1, :]\n",
    "        x = self.layer_norm(last)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "#  Data Helpers\n",
    "def split_data_by_sequence(df, seed):\n",
    "    ids = df[\"seq_ix\"].unique()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    shuffled = rng.permutation(ids)\n",
    "    n_train = int(len(ids) * 0.8)\n",
    "    train_df = df[df[\"seq_ix\"].isin(shuffled[:n_train])]\n",
    "    val_df = df[df[\"seq_ix\"].isin(shuffled[n_train:])]\n",
    "    print(f\"Train sequences: {len(shuffled[:n_train])}, Validation sequences: {len(shuffled[n_train:])}\")\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def create_loaders(train_set, val_set, batch_size, seed):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "#  Training and Evaluation\n",
    "def train_one_epoch(model, loader, criterion, optimizer, clip, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    val_loss = total_loss / len(loader.dataset)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return val_loss, r2\n",
    "\n",
    "\n",
    "#  Training Entry\n",
    "def train_model(config):\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "    df = pd.read_parquet(config[\"paths\"][\"dataset\"])\n",
    "    window_size = config[\"training\"][\"window_size\"]\n",
    "    warmup = config[\"training\"][\"warmup\"]\n",
    "    batch_size = config[\"training\"][\"batch_size\"]\n",
    "    epochs = config[\"training\"][\"epochs\"]\n",
    "    lr = config[\"training\"][\"learning_rate\"]\n",
    "    wd = config[\"training\"][\"weight_decay\"]\n",
    "    clip = config[\"regularization\"][\"gradient_clip\"]\n",
    "    patience = config[\"regularization\"][\"scheduler_patience\"]\n",
    "    factor = config[\"regularization\"][\"scheduler_factor\"]\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    n_features = len(df.columns) - 3\n",
    "    train_df, val_df = split_data_by_sequence(df, config[\"seed\"])\n",
    "    train_set = SequenceDataset(train_df, window_size, warmup)\n",
    "    val_set = SequenceDataset(val_df, window_size, warmup)\n",
    "    train_loader, val_loader = create_loaders(train_set, val_set, batch_size, config[\"seed\"])\n",
    "\n",
    "    model = GRUModel(\n",
    "        input_dim=n_features,\n",
    "        hidden_dim=config[\"model\"][\"hidden_dim\"],\n",
    "        num_layers=config[\"model\"][\"num_layers\"],\n",
    "        dropout=config[\"model\"][\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=factor, patience=patience)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_r2 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, clip, device)\n",
    "        val_loss, val_r2 = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Val R²: {val_r2:.6f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_r2 = val_r2\n",
    "            best_state = model.state_dict().copy()\n",
    "            print(\"New best model found.\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"n_features\": n_features,\n",
    "        \"window_size\": window_size,\n",
    "        \"dropout\": config[\"model\"][\"dropout\"]\n",
    "    }, config[\"paths\"][\"model_checkpoint\"])\n",
    "    print(f\"Model saved to {config['paths']['model_checkpoint']}\")\n",
    "    print(f\"Best Val Loss: {best_val:.6f}, Best Val R²: {best_r2:.6f}\")\n",
    "\n",
    "    from solution import PredictionModel\n",
    "    scorer = ScorerStepByStep(val_df)\n",
    "    prediction_model = PredictionModel(model, window_size, device)\n",
    "    results = scorer.score(prediction_model)\n",
    "    print(f\"Final Mean R² (Scorer): {results['mean_r2']:.6f}\")\n",
    "\n",
    "#  Main\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc600b8-ab3a-423c-b2fa-c46c3f914c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
